{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe und Konfiguration\n",
    "Importiere pandas, numpy, seaborn, matplotlib, sqlalchemy, psycopg2. Setze Plot-Styles, globale Konstanten (Zeitraum 2013-01-01 bis 2025-12-31) und lade DB-DSN aus Umgebungsvariable oder Eingabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Set plot styles\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define global constants\n",
    "START_DATE = \"2013-01-01\"\n",
    "END_DATE = \"2025-12-31\"\n",
    "\n",
    "# Load database DSN from environment variable or user input\n",
    "DB_DSN = os.getenv(\"DB_DSN\") or input(\"Bitte geben Sie den Datenbank-DSN ein: \")\n",
    "\n",
    "# Create a database engine\n",
    "engine = create_engine(DB_DSN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenbankverbindung (SQLAlchemy) testen\n",
    "Erzeuge eine SQLAlchemy Engine (postgresql+psycopg2) und teste eine einfache SELECT 1-Abfrage. Definiere Hilfsfunktion sql_to_df(query, params) für DataFrame-Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the database connection and define a helper function for SQL queries\n",
    "\n",
    "# Test the database connection with a simple SELECT 1 query\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"SELECT 1\")\n",
    "    assert result.scalar() == 1, \"Database connection test failed.\"\n",
    "\n",
    "# Define a helper function to execute SQL queries and return a DataFrame\n",
    "def sql_to_df(query, params=None):\n",
    "    \"\"\"\n",
    "    Execute a SQL query and return the result as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The SQL query to execute.\n",
    "        params (dict, optional): Parameters to pass to the query.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The query result as a DataFrame.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        return pd.read_sql_query(query, connection, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Zeitreihe-Dataset (Median-Tag-Datum je Repo, Repos mit ≥1 Secret, Monats-Binning 2013–2025)\n",
    "CTEs: (1) tag_dates: SELECT repos.id AS repo_id, COALESCE(tags.last_pushed, tags.last_pulled) AS dt JOIN tags→repos; (2) repo_median AS SELECT repo_id, percentile_cont(0.5) WITHIN GROUP (ORDER BY dt) AS med_dt FROM tag_dates GROUP BY repo_id; (3) repo_has_secret: DISTINCT repo_id via repo_layers JOIN layer_secret_fragments; (4) monthly_counts: SELECT date_trunc('month', med_dt) AS mon, COUNT(DISTINCT repo_id) FROM repo_median JOIN repo_has_secret USING(repo_id) WHERE med_dt BETWEEN :start AND :end GROUP BY 1 ORDER BY 1. Lade in DataFrame und fülle fehlende Monate mit 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to generate the Zeitreihe dataset\n",
    "query = \"\"\"\n",
    "WITH tag_dates AS (\n",
    "    SELECT \n",
    "        repos.id AS repo_id, \n",
    "        COALESCE(tags.last_pushed, tags.last_pulled) AS dt\n",
    "    FROM tags\n",
    "    JOIN repos ON tags.repo_name = repos.repo_name\n",
    "),\n",
    "repo_median AS (\n",
    "    SELECT \n",
    "        repo_id, \n",
    "        percentile_cont(0.5) WITHIN GROUP (ORDER BY dt) AS med_dt\n",
    "    FROM tag_dates\n",
    "    GROUP BY repo_id\n",
    "),\n",
    "repo_has_secret AS (\n",
    "    SELECT DISTINCT \n",
    "        repo_id\n",
    "    FROM repo_layers\n",
    "    JOIN layer_secret_fragments ON repo_layers.layer_id = layer_secret_fragments.layer_id\n",
    "),\n",
    "monthly_counts AS (\n",
    "    SELECT \n",
    "        date_trunc('month', med_dt) AS mon, \n",
    "        COUNT(DISTINCT repo_id) AS repo_count\n",
    "    FROM repo_median\n",
    "    JOIN repo_has_secret USING (repo_id)\n",
    "    WHERE med_dt BETWEEN :start AND :end\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    ")\n",
    "SELECT \n",
    "    mon, \n",
    "    COALESCE(repo_count, 0) AS repo_count\n",
    "FROM generate_series(:start::date, :end::date, '1 month') AS mon\n",
    "LEFT JOIN monthly_counts ON mon.mon = monthly_counts.mon\n",
    "ORDER BY mon;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "zeitreihe_df = sql_to_df(query, params={\"start\": START_DATE, \"end\": END_DATE})\n",
    "\n",
    "# Fill missing months with 0 (if not already handled in the query)\n",
    "zeitreihe_df[\"repo_count\"] = zeitreihe_df[\"repo_count\"].fillna(0)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "zeitreihe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot: Zeitgraph Repos mit ≥1 Secret pro Monat\n",
    "Linienplot mit Datum auf x, Anzahl Repos auf y; Achsenbeschriftungen, Limits (2013–2025), Gitter, Rotation der Tick-Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series graph for repositories with ≥1 secret per month\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(zeitreihe_df[\"mon\"], zeitreihe_df[\"repo_count\"], marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Repos with ≥1 Secret\")\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Repositories\", fontsize=12)\n",
    "plt.title(\"Repositories with ≥1 Secret per Month (2013–2025)\", fontsize=14)\n",
    "\n",
    "# Set x-axis limits and rotate tick labels\n",
    "plt.xlim(pd.Timestamp(START_DATE), pd.Timestamp(END_DATE))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Top-10 Secret-Arten (origin)\n",
    "SELECT origin, COUNT(*) AS c FROM secret_fragments WHERE origin IS NOT NULL AND origin <> '' GROUP BY origin ORDER BY c DESC LIMIT 10. In DataFrame laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to get the top 10 secret origins\n",
    "query_top_origins = \"\"\"\n",
    "SELECT \n",
    "    origin, \n",
    "    COUNT(*) AS c \n",
    "FROM secret_fragments \n",
    "WHERE origin IS NOT NULL AND origin <> '' \n",
    "GROUP BY origin \n",
    "ORDER BY c DESC \n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "top_origins_df = sql_to_df(query_top_origins)\n",
    "\n",
    "# Display the DataFrame\n",
    "top_origins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot: Balkendiagramm Top-10 origin\n",
    "Horizontales Barplot der Top-10 origins nach Häufigkeit, sortiert absteigend, Wertebeschriftungen hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the horizontal bar chart for the top 10 origins\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"c\", \n",
    "    y=\"origin\", \n",
    "    data=top_origins_df, \n",
    "    palette=\"viridis\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.ylabel(\"Origin\", fontsize=12)\n",
    "plt.title(\"Top 10 Origins by Frequency\", fontsize=14)\n",
    "\n",
    "# Add value annotations to each bar\n",
    "for index, value in enumerate(top_origins_df[\"c\"]):\n",
    "    plt.text(value + 0.5, index, str(value), va=\"center\", fontsize=10)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Secret-Wiederholungen über Repos und über Layer+Repos\n",
    "CTE base: secret_fragments sf JOIN layer_secret_fragments lsf USING(fragment_hash) JOIN repo_layers rl USING(layer_id) JOIN repos r ON r.id=rl.repo_id. Aggregiere pro identischem Secret-Text sf.secret: repo_cnt = COUNT(DISTINCT r.id), layer_repo_cnt = COUNT(DISTINCT (rl.layer_id, r.id)). Erzeuge Schwellen-Buckets (>5, >10, >100, >1000) und zähle Anzahl Secrets je Bucket getrennt für repo_cnt und layer_repo_cnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to calculate secret repetition thresholds\n",
    "query_secret_repeats = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        sf.secret,\n",
    "        COUNT(DISTINCT r.id) AS repo_cnt,\n",
    "        COUNT(DISTINCT (rl.layer_id, r.id)) AS layer_repo_cnt\n",
    "    FROM secret_fragments sf\n",
    "    JOIN layer_secret_fragments lsf USING (fragment_hash)\n",
    "    JOIN repo_layers rl USING (layer_id)\n",
    "    JOIN repos r ON r.id = rl.repo_id\n",
    "    GROUP BY sf.secret\n",
    "),\n",
    "thresholds AS (\n",
    "    SELECT \n",
    "        'repo_cnt' AS category,\n",
    "        CASE \n",
    "            WHEN repo_cnt > 1000 THEN '>1000'\n",
    "            WHEN repo_cnt > 100 THEN '>100'\n",
    "            WHEN repo_cnt > 10 THEN '>10'\n",
    "            WHEN repo_cnt > 5 THEN '>5'\n",
    "            ELSE NULL\n",
    "        END AS bucket,\n",
    "        COUNT(*) AS secret_count\n",
    "    FROM base\n",
    "    GROUP BY 1, 2\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'layer_repo_cnt' AS category,\n",
    "        CASE \n",
    "            WHEN layer_repo_cnt > 1000 THEN '>1000'\n",
    "            WHEN layer_repo_cnt > 100 THEN '>100'\n",
    "            WHEN layer_repo_cnt > 10 THEN '>10'\n",
    "            WHEN layer_repo_cnt > 5 THEN '>5'\n",
    "            ELSE NULL\n",
    "        END AS bucket,\n",
    "        COUNT(*) AS secret_count\n",
    "    FROM base\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "SELECT \n",
    "    category, \n",
    "    bucket, \n",
    "    COALESCE(secret_count, 0) AS secret_count\n",
    "FROM thresholds\n",
    "WHERE bucket IS NOT NULL\n",
    "ORDER BY category, bucket;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "secret_repeats_df = sql_to_df(query_secret_repeats)\n",
    "\n",
    "# Pivot the DataFrame for easier plotting\n",
    "pivot_df = secret_repeats_df.pivot(index=\"bucket\", columns=\"category\", values=\"secret_count\").fillna(0)\n",
    "\n",
    "# Plot the bar chart for secret repetition thresholds\n",
    "pivot_df.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(12, 6),\n",
    "    color=[\"#1f77b4\", \"#ff7f0e\"],\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Threshold Buckets\", fontsize=12)\n",
    "plt.ylabel(\"Number of Secrets\", fontsize=12)\n",
    "plt.title(\"Secret Repetition Thresholds Across Repositories and Layers+Repositories\", fontsize=14)\n",
    "\n",
    "# Add legend and adjust layout\n",
    "plt.legend(title=\"Category\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot: Balkendiagramm Schwellen (>5, >10, >100, >1000), gruppiert (Repos vs. Layer+Repos)\n",
    "Erzeuge gruppiertes Balkendiagramm je Schwellenwert mit zwei Balken: 'Distinct Repos' und 'Distinct Layer+Repos'. Beschriftungen und Legende hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query to calculate secret repetition thresholds\n",
    "query_secret_repeats = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        sf.secret,\n",
    "        COUNT(DISTINCT r.id) AS repo_cnt,\n",
    "        COUNT(DISTINCT (rl.layer_id, r.id)) AS layer_repo_cnt\n",
    "    FROM secret_fragments sf\n",
    "    JOIN layer_secret_fragments lsf USING (fragment_hash)\n",
    "    JOIN repo_layers rl USING (layer_id)\n",
    "    JOIN repos r ON r.id = rl.repo_id\n",
    "    GROUP BY sf.secret\n",
    "),\n",
    "thresholds AS (\n",
    "    SELECT \n",
    "        'repo_cnt' AS category,\n",
    "        CASE \n",
    "            WHEN repo_cnt > 1000 THEN '>1000'\n",
    "            WHEN repo_cnt > 100 THEN '>100'\n",
    "            WHEN repo_cnt > 10 THEN '>10'\n",
    "            WHEN repo_cnt > 5 THEN '>5'\n",
    "            ELSE NULL\n",
    "        END AS bucket,\n",
    "        COUNT(*) AS secret_count\n",
    "    FROM base\n",
    "    GROUP BY 1, 2\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'layer_repo_cnt' AS category,\n",
    "        CASE \n",
    "            WHEN layer_repo_cnt > 1000 THEN '>1000'\n",
    "            WHEN layer_repo_cnt > 100 THEN '>100'\n",
    "            WHEN layer_repo_cnt > 10 THEN '>10'\n",
    "            WHEN layer_repo_cnt > 5 THEN '>5'\n",
    "            ELSE NULL\n",
    "        END AS bucket,\n",
    "        COUNT(*) AS secret_count\n",
    "    FROM base\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "SELECT \n",
    "    category, \n",
    "    bucket, \n",
    "    COALESCE(secret_count, 0) AS secret_count\n",
    "FROM thresholds\n",
    "WHERE bucket IS NOT NULL\n",
    "ORDER BY category, bucket;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "secret_repeats_df = sql_to_df(query_secret_repeats)\n",
    "\n",
    "# Pivot the DataFrame for easier plotting\n",
    "pivot_df = secret_repeats_df.pivot(index=\"bucket\", columns=\"category\", values=\"secret_count\").fillna(0)\n",
    "\n",
    "# Plot the bar chart for secret repetition thresholds\n",
    "pivot_df.plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(12, 6),\n",
    "    color=[\"#1f77b4\", \"#ff7f0e\"],\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Threshold Buckets\", fontsize=12)\n",
    "plt.ylabel(\"Number of Secrets\", fontsize=12)\n",
    "plt.title(\"Secret Repetition Thresholds Across Repositories and Layers+Repositories\", fontsize=14)\n",
    "\n",
    "# Add legend and adjust layout\n",
    "plt.legend(title=\"Category\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query: Feature-Matrix pro Secret (Join secret_fragments ↔ layer_secret_fragments ↔ layer_data ↔ repo_layers ↔ repos)\n",
    "Baue Feature-Tabelle auf Secret-Ebene (Gruppierung nach sf.secret): Features: median(sf.file_size) AS file_size, mode(sf.file_type) AS file_type; avg(ld.file_count) AS file_count, avg(ld.max_depth) AS max_depth, avg(ld.uncompressed_size) AS uncompressed_size; aus repos: avg(r.pull_count) AS pull_count, avg(CASE WHEN r.is_automated THEN 1 ELSE 0 END) AS is_automated, avg(CASE WHEN r.is_official THEN 1 ELSE 0 END) AS is_official, avg(r.star_count) AS star_count; Zielvariable: repo_occurrences = COUNT(DISTINCT r.id). Lade in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feature_matrix = \"\"\"\n",
    "WITH secret_features AS (\n",
    "    SELECT \n",
    "        sf.secret,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sf.file_size) AS file_size,\n",
    "        MODE() WITHIN GROUP (ORDER BY sf.file_type) AS file_type,\n",
    "        AVG(ld.file_count) AS file_count,\n",
    "        AVG(ld.max_depth) AS max_depth,\n",
    "        AVG(ld.uncompressed_size) AS uncompressed_size,\n",
    "        AVG(r.pull_count) AS pull_count,\n",
    "        AVG(CASE WHEN r.is_automated THEN 1 ELSE 0 END) AS is_automated,\n",
    "        AVG(CASE WHEN r.is_official THEN 1 ELSE 0 END) AS is_official,\n",
    "        AVG(r.star_count) AS star_count,\n",
    "        COUNT(DISTINCT r.id) AS repo_occurrences\n",
    "    FROM secret_fragments sf\n",
    "    JOIN layer_secret_fragments lsf USING (fragment_hash)\n",
    "    JOIN layer_data ld USING (layer_id)\n",
    "    JOIN repo_layers rl USING (layer_id)\n",
    "    JOIN repos r ON r.id = rl.repo_id\n",
    "    GROUP BY sf.secret\n",
    ")\n",
    "SELECT * FROM secret_features;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "feature_matrix_df = sql_to_df(query_feature_matrix)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "feature_matrix_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Zielvariable und Encoding\n",
    "Erzeuge y = log1p(repo_occurrences). One-Hot-Encoding für file_type (Top-k Kategorien, Rest als 'other'), skaliere numerische Features optional. Entferne Zeilen mit fehlenden Kernwerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Define the target variable y as log1p(repo_occurrences)\n",
    "feature_matrix_df[\"y\"] = np.log1p(feature_matrix_df[\"repo_occurrences\"])\n",
    "\n",
    "# One-Hot-Encoding for file_type (Top-k categories, rest as 'other')\n",
    "top_k = 10\n",
    "top_categories = feature_matrix_df[\"file_type\"].value_counts().nlargest(top_k).index\n",
    "feature_matrix_df[\"file_type\"] = feature_matrix_df[\"file_type\"].apply(\n",
    "    lambda x: x if x in top_categories else \"other\"\n",
    ")\n",
    "\n",
    "# Define numerical features to scale\n",
    "numerical_features = [\n",
    "    \"file_size\",\n",
    "    \"file_count\",\n",
    "    \"max_depth\",\n",
    "    \"uncompressed_size\",\n",
    "    \"pull_count\",\n",
    "    \"star_count\",\n",
    "]\n",
    "\n",
    "# Define categorical features for encoding\n",
    "categorical_features = [\"file_type\"]\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]), numerical_features),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")), (\"onehot\", OneHotEncoder(drop=\"first\"))]), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the feature matrix\n",
    "X = preprocessor.fit_transform(feature_matrix_df)\n",
    "\n",
    "# Remove rows with missing core values\n",
    "feature_matrix_df.dropna(subset=[\"repo_occurrences\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korrelationen (Pearson und Spearman) berechnen und rangieren\n",
    "Berechne für alle Features die Pearson- und Spearman-Korrelation mit y. Erzeuge DataFrame mit Feature, pearson_r, spearman_r, abs_pearson, abs_spearman und sortiere nach abs_spearman (oder kombiniertem Score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate Pearson and Spearman correlations for all features with the target variable y\n",
    "correlation_results = []\n",
    "for feature in numerical_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)):\n",
    "    if feature in feature_matrix_df.columns:\n",
    "        pearson_r, _ = stats.pearsonr(feature_matrix_df[feature], feature_matrix_df[\"y\"])\n",
    "        spearman_r, _ = stats.spearmanr(feature_matrix_df[feature], feature_matrix_df[\"y\"])\n",
    "        correlation_results.append({\n",
    "            \"feature\": feature,\n",
    "            \"pearson_r\": pearson_r,\n",
    "            \"spearman_r\": spearman_r,\n",
    "            \"abs_pearson\": abs(pearson_r),\n",
    "            \"abs_spearman\": abs(spearman_r)\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# Sort the DataFrame by absolute Spearman correlation (or combined score)\n",
    "correlation_df.sort_values(by=\"abs_spearman\", ascending=False, inplace=True)\n",
    "\n",
    "# Display the top correlations\n",
    "correlation_df.head()\n",
    "\n",
    "# Plot the top features by absolute Spearman correlation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"abs_spearman\",\n",
    "    y=\"feature\",\n",
    "    data=correlation_df.head(10),\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Absolute Spearman Correlation\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Top 10 Features by Absolute Spearman Correlation with Target (y)\", fontsize=14)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot: Balkendiagramm der absoluten Korrelationskoeffizienten\n",
    "Barplot der Top-Features nach |Spearman| (optional nebeneinander mit |Pearson|). Achsenbeschriftungen und Legende hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the top features by absolute Spearman correlation alongside Pearson correlation\n",
    "top_features = correlation_df.head(10)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.4\n",
    "x = range(len(top_features))\n",
    "\n",
    "# Plot Spearman correlations\n",
    "plt.barh(\n",
    "    y=[i - bar_width / 2 for i in x],\n",
    "    width=top_features[\"abs_spearman\"],\n",
    "    height=bar_width,\n",
    "    color=\"skyblue\",\n",
    "    label=\"|Spearman|\"\n",
    ")\n",
    "\n",
    "# Plot Pearson correlations\n",
    "plt.barh(\n",
    "    y=[i + bar_width / 2 for i in x],\n",
    "    width=top_features[\"abs_pearson\"],\n",
    "    height=bar_width,\n",
    "    color=\"salmon\",\n",
    "    label=\"|Pearson|\"\n",
    ")\n",
    "\n",
    "# Add feature names as y-axis labels\n",
    "plt.yticks(x, top_features[\"feature\"], fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Absolute Correlation\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Top 10 Features by Absolute Correlation with Target (y)\", fontsize=14)\n",
    "\n",
    "# Add legend and adjust layout\n",
    "plt.legend(title=\"Correlation Type\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Ergebnisse/Abbildungen speichern\n",
    "Speichere Plots als PNG/SVG und exportiere aggregierte Tabellen (Zeitreihe, Top-10, Schwellen, Korrelationen) als CSV in ein Ausgabe-Verzeichnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save Zeitreihe plot as PNG and SVG\n",
    "zeitreihe_plot_path_png = os.path.join(output_dir, \"zeitreihe_repos_with_secrets.png\")\n",
    "zeitreihe_plot_path_svg = os.path.join(output_dir, \"zeitreihe_repos_with_secrets.svg\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(zeitreihe_df[\"mon\"], zeitreihe_df[\"repo_count\"], marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Repos with ≥1 Secret\")\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Repositories\", fontsize=12)\n",
    "plt.title(\"Repositories with ≥1 Secret per Month (2013–2025)\", fontsize=14)\n",
    "plt.xlim(pd.Timestamp(START_DATE), pd.Timestamp(END_DATE))\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(zeitreihe_plot_path_png)\n",
    "plt.savefig(zeitreihe_plot_path_svg)\n",
    "plt.close()\n",
    "\n",
    "# Save Top 10 Origins plot as PNG and SVG\n",
    "top_origins_plot_path_png = os.path.join(output_dir, \"top_10_origins.png\")\n",
    "top_origins_plot_path_svg = os.path.join(output_dir, \"top_10_origins.svg\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"c\", y=\"origin\", data=top_origins_df, palette=\"viridis\")\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.ylabel(\"Origin\", fontsize=12)\n",
    "plt.title(\"Top 10 Origins by Frequency\", fontsize=14)\n",
    "for index, value in enumerate(top_origins_df[\"c\"]):\n",
    "    plt.text(value + 0.5, index, str(value), va=\"center\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(top_origins_plot_path_png)\n",
    "plt.savefig(top_origins_plot_path_svg)\n",
    "plt.close()\n",
    "\n",
    "# Save Secret Repetition Thresholds plot as PNG and SVG\n",
    "secret_repeats_plot_path_png = os.path.join(output_dir, \"secret_repetition_thresholds.png\")\n",
    "secret_repeats_plot_path_svg = os.path.join(output_dir, \"secret_repetition_thresholds.svg\")\n",
    "pivot_df.plot(kind=\"bar\", figsize=(12, 6), color=[\"#1f77b4\", \"#ff7f0e\"], edgecolor=\"black\")\n",
    "plt.xlabel(\"Threshold Buckets\", fontsize=12)\n",
    "plt.ylabel(\"Number of Secrets\", fontsize=12)\n",
    "plt.title(\"Secret Repetition Thresholds Across Repositories and Layers+Repositories\", fontsize=14)\n",
    "plt.legend(title=\"Category\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(secret_repeats_plot_path_png)\n",
    "plt.savefig(secret_repeats_plot_path_svg)\n",
    "plt.close()\n",
    "\n",
    "# Save Correlation plots as PNG and SVG\n",
    "correlation_plot_path_png = os.path.join(output_dir, \"top_features_correlation.png\")\n",
    "correlation_plot_path_svg = os.path.join(output_dir, \"top_features_correlation.svg\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.4\n",
    "x = range(len(top_features))\n",
    "plt.barh(y=[i - bar_width / 2 for i in x], width=top_features[\"abs_spearman\"], height=bar_width, color=\"skyblue\", label=\"|Spearman|\")\n",
    "plt.barh(y=[i + bar_width / 2 for i in x], width=top_features[\"abs_pearson\"], height=bar_width, color=\"salmon\", label=\"|Pearson|\")\n",
    "plt.yticks(x, top_features[\"feature\"], fontsize=10)\n",
    "plt.xlabel(\"Absolute Correlation\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Top 10 Features by Absolute Correlation with Target (y)\", fontsize=14)\n",
    "plt.legend(title=\"Correlation Type\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(correlation_plot_path_png)\n",
    "plt.savefig(correlation_plot_path_svg)\n",
    "plt.close()\n",
    "\n",
    "# Save DataFrames as CSV\n",
    "zeitreihe_csv_path = os.path.join(output_dir, \"zeitreihe_repos_with_secrets.csv\")\n",
    "top_origins_csv_path = os.path.join(output_dir, \"top_10_origins.csv\")\n",
    "secret_repeats_csv_path = os.path.join(output_dir, \"secret_repetition_thresholds.csv\")\n",
    "correlation_csv_path = os.path.join(output_dir, \"top_features_correlation.csv\")\n",
    "\n",
    "zeitreihe_df.to_csv(zeitreihe_csv_path, index=False)\n",
    "top_origins_df.to_csv(top_origins_csv_path, index=False)\n",
    "secret_repeats_df.to_csv(secret_repeats_csv_path, index=False)\n",
    "correlation_df.to_csv(correlation_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
