{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from scipy.stats import ks_2samp, chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Validate sample vs. full dataset (bias, KS/ChiÂ² tests).\")\n",
    "    p.add_argument(\"--digest-analysis\", required=True, help=\"Path to digest_analysis.jsonl (sample)\")\n",
    "    p.add_argument(\"--unique-repos\", help=\"Path to unique_repos.jsonl (full; fallback if no precomputed CSV)\")\n",
    "    p.add_argument(\"--combined-tags\", help=\"Path to combined_tags.jsonl (full; fallback if no precomputed CSV)\")\n",
    "    p.add_argument(\"--db-url\", default=os.environ.get(\"DATABASE_URL\", \"\"), help=\"Postgres URL for sample DB\")\n",
    "    p.add_argument(\"--outdir\", default=\"analysis_outputs\", help=\"Output directory for CSVs\")\n",
    "    p.add_argument(\"--precomputed-dir\", default=\"\", help=\"Directory containing precomputed CSVs from Go\")\n",
    "    p.add_argument(\"--max-ks-samples\", type=int, default=500000, help=\"Max reservoir sample size for JSONL fallback\")\n",
    "    return p.parse_args()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def ensure_outdir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def month_str(ts: pd.Timestamp) -> str:\n",
    "    ts = pd.to_datetime(ts)\n",
    "    return f\"{ts.year:04d}-{ts.month:02d}\"\n",
    "\n",
    "def read_hist_csv(path: str) -> Counter:\n",
    "    df = pd.read_csv(path)\n",
    "    if df.empty or \"key\" not in df or \"count\" not in df:\n",
    "        return Counter()\n",
    "    return Counter({str(k): int(v) for k, v in zip(df[\"key\"], df[\"count\"])})\n",
    "\n",
    "def read_series_csv(path: str) -> np.ndarray:\n",
    "    df = pd.read_csv(path)\n",
    "    if \"value\" not in df or df.empty:\n",
    "        return np.array([], dtype=float)\n",
    "    return df[\"value\"].astype(float).to_numpy()\n",
    "\n",
    "def normalize_cat(s):\n",
    "    if s is None:\n",
    "        return \"unknown\"\n",
    "    s = str(s).strip()\n",
    "    return s if s else \"unknown\"\n",
    "\n",
    "def make_bias_table(full_counts: Counter, sample_counts: Counter) -> pd.DataFrame:\n",
    "    keys = sorted(set(full_counts.keys()) | set(sample_counts.keys()))\n",
    "    rows = []\n",
    "    n_full = sum(full_counts.values())\n",
    "    n_sam = sum(sample_counts.values())\n",
    "    for k in keys:\n",
    "        cf = full_counts.get(k, 0)\n",
    "        cs = sample_counts.get(k, 0)\n",
    "        pf = cf / n_full if n_full > 0 else 0.0\n",
    "        ps = cs / n_sam if n_sam > 0 else 0.0\n",
    "        bias = (pf / ps) if ps > 0 else (np.inf if pf > 0 else 1.0)\n",
    "        rows.append({\"key\": k, \"count_full\": cf, \"count_sample\": cs, \"p_full\": pf, \"p_sample\": ps, \"bias_full_over_sample\": bias})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def make_numeric_bias_bins(full_vals: np.ndarray, sample_vals: np.ndarray, nbins: int = 20) -> pd.DataFrame:\n",
    "    full_vals = full_vals[~np.isnan(full_vals)]\n",
    "    sample_vals = sample_vals[~np.isnan(sample_vals)]\n",
    "    if len(full_vals) == 0 or len(sample_vals) == 0:\n",
    "        return pd.DataFrame(columns=[\"bin_left\",\"bin_right\",\"count_full\",\"count_sample\",\"p_full\",\"p_sample\",\"bias_full_over_sample\"])\n",
    "    qs = np.linspace(0, 1, nbins + 1)\n",
    "    edges = np.quantile(full_vals, qs)\n",
    "    edges = np.unique(edges)\n",
    "    if len(edges) < 3:\n",
    "        edges = np.linspace(np.nanmin(full_vals), np.nanmax(full_vals), nbins + 1)\n",
    "    cf, _ = np.histogram(full_vals, bins=edges)\n",
    "    cs, _ = np.histogram(sample_vals, bins=edges)\n",
    "    n_full, n_sam = cf.sum(), cs.sum()\n",
    "    rows = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        pf = cf[i] / n_full if n_full > 0 else 0.0\n",
    "        ps = cs[i] / n_sam if n_sam > 0 else 0.0\n",
    "        bias = (pf / ps) if ps > 0 else (np.inf if pf > 0 else 1.0)\n",
    "        rows.append({\n",
    "            \"bin_left\": edges[i],\n",
    "            \"bin_right\": edges[i+1],\n",
    "            \"count_full\": int(cf[i]),\n",
    "            \"count_sample\": int(cs[i]),\n",
    "            \"p_full\": pf,\n",
    "            \"p_sample\": ps,\n",
    "            \"bias_full_over_sample\": bias,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def save_series_counts(counts: Counter, out_path: str):\n",
    "    total = sum(counts.values())\n",
    "    rows = []\n",
    "    for key, n in sorted(counts.items(), key=lambda kv: kv[0]):\n",
    "        p = n / total if total > 0 else 0.0\n",
    "        rows.append({\"key\": key, \"count\": n, \"proportion\": p})\n",
    "    pd.DataFrame(rows).to_csv(out_path, index=False)\n",
    "\n",
    "def compute_unique_layer_share_from_digest(digest_analysis_path):\n",
    "    total = 0\n",
    "    uniq = 0\n",
    "    with open(digest_analysis_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            total += 1\n",
    "            rc = obj.get(\"repo_count\")\n",
    "            if rc is None:\n",
    "                repos = obj.get(\"repos\") or []\n",
    "                rc = len(repos)\n",
    "            if rc == 1:\n",
    "                uniq += 1\n",
    "    share = (uniq / total) if total > 0 else math.nan\n",
    "    return total, uniq, share\n",
    "\n",
    "# ---------- FULL (prefer precomputed CSVs) ----------\n",
    "\n",
    "def load_full_tags(pre_dir: str, combined_tags_path: str, max_ks_samples: int):\n",
    "    if pre_dir:\n",
    "        lp_month = read_hist_csv(os.path.join(pre_dir, \"full_last_pushed_month.csv\"))\n",
    "        status = read_hist_csv(os.path.join(pre_dir, \"full_status.csv\"))\n",
    "        lp_epochs = read_series_csv(os.path.join(pre_dir, \"full_last_pushed_epoch_reservoir.csv\"))\n",
    "        size = read_series_csv(os.path.join(pre_dir, \"full_size_reservoir.csv\"))\n",
    "        if sum(lp_month.values()) > 0 and len(lp_epochs) > 0 and len(size) > 0 and sum(status.values()) > 0:\n",
    "            return {\n",
    "                \"last_pushed_month_counts\": lp_month,\n",
    "                \"last_pushed_epochs_sample\": lp_epochs.astype(float),\n",
    "                \"size_sample\": size.astype(float),\n",
    "                \"status_counts\": status,\n",
    "            }\n",
    "        # else: fall through to JSONL\n",
    "    # JSONL fallback (slow)\n",
    "    last_pushed_month_counts = Counter()\n",
    "    last_pushed_reservoir = []\n",
    "    size_reservoir = []\n",
    "    status_counts = Counter()\n",
    "    n_last = 0\n",
    "    n_size = 0\n",
    "    rng = np.random.default_rng(42)\n",
    "    with open(combined_tags_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            lp = obj.get(\"last_pushed\")\n",
    "            if lp:\n",
    "                ts = pd.to_datetime(lp, utc=True, errors=\"coerce\")\n",
    "                if pd.notna(ts):\n",
    "                    last_pushed_month_counts[month_str(ts)] += 1\n",
    "                    # reservoir\n",
    "                    if n_last < max_ks_samples:\n",
    "                        last_pushed_reservoir.append(ts.value / 1e9)\n",
    "                        n_last += 1\n",
    "                    else:\n",
    "                        j = rng.integers(0, n_last + 1)\n",
    "                        if j < max_ks_samples:\n",
    "                            last_pushed_reservoir[j] = ts.value / 1e9\n",
    "                        n_last += 1\n",
    "            sz = obj.get(\"size\", None)\n",
    "            if sz is not None:\n",
    "                try:\n",
    "                    val = float(sz)\n",
    "                    if n_size < max_ks_samples:\n",
    "                        size_reservoir.append(val)\n",
    "                        n_size += 1\n",
    "                    else:\n",
    "                        j = rng.integers(0, n_size + 1)\n",
    "                        if j < max_ks_samples:\n",
    "                            size_reservoir[j] = val\n",
    "                        n_size += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            status = normalize_cat(obj.get(\"status\"))\n",
    "            status_counts[status] += 1\n",
    "    return {\n",
    "        \"last_pushed_month_counts\": last_pushed_month_counts,\n",
    "        \"last_pushed_epochs_sample\": np.array(last_pushed_reservoir, dtype=float),\n",
    "        \"size_sample\": np.array(size_reservoir, dtype=float),\n",
    "        \"status_counts\": status_counts,\n",
    "    }\n",
    "\n",
    "def load_full_repos(pre_dir: str, unique_repos_path: str, max_ks_samples: int):\n",
    "    if pre_dir:\n",
    "        io_counts = read_hist_csv(os.path.join(pre_dir, \"full_is_official.csv\"))\n",
    "        pull = read_series_csv(os.path.join(pre_dir, \"full_pull_count_reservoir.csv\"))\n",
    "        if sum(io_counts.values()) > 0 and len(pull) > 0:\n",
    "            return {\n",
    "                \"pull_count_sample\": pull.astype(float),\n",
    "                \"is_official_counts\": io_counts,\n",
    "            }\n",
    "        # else: fall through\n",
    "    # JSONL fallback (slow)\n",
    "    pull_reservoir = []\n",
    "    n_pull = 0\n",
    "    is_official_counts = Counter()\n",
    "    rng = np.random.default_rng(43)\n",
    "    with open(unique_repos_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            pc = obj.get(\"pull_count\")\n",
    "            if pc is not None:\n",
    "                try:\n",
    "                    val = float(pc)\n",
    "                    if n_pull < max_ks_samples:\n",
    "                        pull_reservoir.append(val)\n",
    "                        n_pull += 1\n",
    "                    else:\n",
    "                        j = rng.integers(0, n_pull + 1)\n",
    "                        if j < max_ks_samples:\n",
    "                            pull_reservoir[j] = val\n",
    "                        n_pull += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            iso = bool(obj.get(\"is_official\"))\n",
    "            is_official_counts[\"official\" if iso else \"unofficial\"] += 1\n",
    "    return {\n",
    "        \"pull_count_sample\": np.array(pull_reservoir, dtype=float),\n",
    "        \"is_official_counts\": is_official_counts,\n",
    "    }\n",
    "\n",
    "# ---------- SAMPLE (DB) ----------\n",
    "\n",
    "def load_sample_from_db(db_url):\n",
    "    engine = create_engine(db_url)\n",
    "    with engine.connect() as conn:\n",
    "        # last_pushed\n",
    "        df_lp = pd.read_sql_query(text(\"SELECT last_pushed FROM tags WHERE last_pushed IS NOT NULL\"), conn)\n",
    "        # ns -> s\n",
    "        lp_epochs = pd.to_datetime(df_lp[\"last_pushed\"]).astype(\"int64\").to_numpy(dtype=float) / 1e9\n",
    "        df_lp[\"month_str\"] = pd.to_datetime(df_lp[\"last_pushed\"]).map(month_str)\n",
    "        month_counts = Counter(df_lp[\"month_str\"].tolist())\n",
    "\n",
    "        # size\n",
    "        df_sz = pd.read_sql_query(text(\"SELECT size FROM tags WHERE size IS NOT NULL\"), conn)\n",
    "        size_vals = df_sz[\"size\"].astype(float).to_numpy()\n",
    "\n",
    "        # status\n",
    "        df_st = pd.read_sql_query(text(\"SELECT COALESCE(NULLIF(TRIM(status), ''), 'unknown') AS status FROM tags\"), conn)\n",
    "        status_counts = Counter(df_st[\"status\"].tolist())\n",
    "\n",
    "        # pull_count\n",
    "        df_pc = pd.read_sql_query(text(\"SELECT pull_count FROM repositories WHERE pull_count IS NOT NULL\"), conn)\n",
    "        pull_vals = df_pc[\"pull_count\"].astype(float).to_numpy()\n",
    "\n",
    "        # is_official\n",
    "        df_io = pd.read_sql_query(text(\"SELECT CASE WHEN is_official THEN 'official' ELSE 'unofficial' END AS cat FROM repositories\"), conn)\n",
    "        io_counts = Counter(df_io[\"cat\"].tolist())\n",
    "\n",
    "    return {\n",
    "        \"last_pushed_epochs\": lp_epochs,\n",
    "        \"last_pushed_month_counts\": month_counts,\n",
    "        \"size_vals\": size_vals,\n",
    "        \"status_counts\": status_counts,\n",
    "        \"pull_vals\": pull_vals,\n",
    "        \"is_official_counts\": io_counts,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
